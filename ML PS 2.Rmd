---
title: "HU ML Problem Set 2"
author: "Aaron"
date: "September 22, 2016"
output: 
  html_document: 
    highlight: textmate
    theme: cosmo
---

### Instructions:

> Complete the following problems from the Kelleher book (Chapter 4) 2, 3, 4

### Calling helpers

```{r, message=FALSE, warning=FALSE}

library(tidyverse)
library(readxl)
library(knitr)
library(data.tree)
library(ggtree)
library(magrittr)

pb_1 = read_excel("../data/homework2_data.xlsx", 1)
pb_2 = read_excel("../data/homework2_data.xlsx", 2)
pb_3 = read_excel("../data/homework2_data.xlsx", 3)

```

### Define Helper Functions:

**Define `entropy` function**

```{r}

entropy <- function(target) {
    
    freq <- table(target)/length(target)
    vec <- as.data.frame(freq)[,2] 
    vec<-vec[vec>0]
    -sum(vec * log2(vec))
    
}

```

**Define `information_gain` function**

```{r}

IG_numeric<-function(data, feature, target, bins=4) {
    #Strip out rows where feature is NA
    data<-data[!is.na(data[,feature]),]
    #compute entropy for the parent
    e0 <-entropy(data[,target])
    
    data$cat<-cut(data[,feature], breaks=bins, labels=c(1:bins))
    
    dd_data<-ddply(data, "cat", here(summarise), 
                   e=entropy(get(target)), 
                   N=length(get(target)),
                   min=min(get(feature)),
                   max=max(get(feature))
    )
    
    #calculate p for each value of feature
    dd_data$p<-dd_data$N/nrow(data)
    #compute IG
    IG<-e0-sum(dd_data$p*dd_data$e)
    
    return(IG)
}


#returns IG for categorical variables.
IG_cat<-function(data,feature,target){
    #Strip out rows where feature is NA
    data<-data[!is.na(data[,feature]),] 
    #use ddply to compute e and p for each value of the feature
    dd_data<-ddply(data, feature, here(summarise), e=entropy(get(target)), N=length(get(target)))
    #compute entropy for the parent
    e0<-entropy(data[,target])
    #calculate p for each value of feature
    dd_data$p<-dd_data$N/nrow(data)
    #compute IG
    IG<-e0-sum(dd_data$p*dd_data$e)
    
    return(IG)
}

```


### Problem 2

A convicted criminal who reoffends after release is known as a $recidivist$. The table below lists a dataset that describes prisoners released on parole, and whether they reoffended within two years of release.

```{r}

pb_1 %>% kable()

```

This dataset lists six instances where prisoners were granted parole. Each of these instances are described in terms of three binary descriptive features **(`GOOD BEHAVIOR`, AGE < 30, DRUG DEPENDENT)** and a binary target feature, `RECIDIVIST`. The `GOOD BEHAVIOR` feature has a value of true if the prisoner had not committed any infringements during incarceration, the `AGE` < 30 has a value of true if the prisoner was under 30 years of age when granted parole, and the DRUG DEPENDENT feature is true if the prisoner had a drug addiction at the time of parole. The target feature, `RECIDIVIST`, has a true value if the prisoner was arrested within two years of being released; otherwise it has a value of false. 


- Using this dataset, construct the decision tree that would be generated by the **ID3 algorithm**, using entropy-based information gain. 


The first step in building the decision tree is to figure out which of the three descriptive features is the best one on which to split the dataset at the root node (i.e., which descriptive feature has the highest information gain). The total entropy for this dataset is computed as follows:


```{r}

entropy(pb_1$recidivist)

```

Then get the information gain for the three feature 

```{r}

IG_cat(pb_1, "age_less_30", "recidivist")

```



- What prediction will the decision tree generated in part (a) of this question return for the following query? `GOOD BEHAVIOR` = false, `AGE` < 30 = false, `DRUG DEPENDENT` = true 

- What prediction will the decision tree generated in part (a) of this question return for the following query? `GOOD BEHAVIOR` = true, `AGE` < 30 = true, `DRUG DEPENDENT` = false


### Problem 3

The table below lists a sample of data from a census.

```{r}

pb_2 %>% kable()

```

There are four descriptive features and one target feature in this dataset: 

- `AGE`, a continuous feature listing the age of the individual 
- `EDUCATION`, a categorical feature listing the highest education award achieved by the individual (high school, bachelors, doctorate) 
- `MARITAL STATUS` (never married, married, divorced)
- `OCCUPATION` (transport = works in the transportation industry; professional = doctors, lawyers, etc.; agriculture = works in the agricultural industry; armed forces = is a member of the armed forces) 
- `ANNUAL INCOME`, the target feature with 3 levels (< 25K, 25Kâ€“ 50K, > 50K) 

- Calculate the **entropy** for this dataset. 

```{r}

pb_2 %>% kable()

```


```{r}

```


- Calculate the **Gini index** for this dataset. 

- When building a decision tree, the easiest way to handle a continuous feature is to define a threshold around which splits will be made. What would be the optimal threshold to split the continuous AGE feature (use information gain based on entropy as the feature selection measure)? 

- Calculate **information gain** (based on entropy) for the `EDUCATION`, `MARITAL STATUS`, and `OCCUPATION` features. 

- Calculate the **information gain ratio** (based on entropy) for `EDUCATION`, `MARITAL STATUS`, and `OCCUPATION` features. 

- Calculate **information gain** using the **Gini index** for the `EDUCATION`, `MARITAL STATUS`, and `OCCUPATION` features.


### Problem 4

```{r}

pb_3 %>% kable()

```


The diagram below shows a decision tree for the task of predicting heart disease. 33 The descriptive features in this domain describe whether the patient suffers from chest pain (`CHEST PAIN`) as well as the blood pressure of the patient (`BLOOD PRESSURE`). The binary target feature is `HEART DISEASE`. The table beside the diagram lists a pruning set from this domain.

![](../img/image_4.png)

Using the pruning set, apply **reduced error pruning** to the decision tree. Assume that the algorithm is applied in a bottom-up, left-to-right fashion. For each iteration of the algorithm, indicate the subtrees considered as pruning candidates, explain why the algorithm chooses to prune or leave these subtrees in the tree, and illustrate the tree that results from each iteration.


